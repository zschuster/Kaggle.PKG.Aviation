---
title: "Reducing Commercial Aviation Fatalities"
author: "Zach Schuster"
date: "January 16, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all()
require(xgboost)
library(data.table)
```


First, read in the data

```{r}

readAndSplit(path = "data/train.csv",
             target_name = "event",
             name = "train")

# x_train = fread("data/train.csv")

```


for now, remove the experiment as the test set has a totally different experiment!

```{r}
x_train[, experiment := NULL]

```


coerce proper variables to categorical
```{r}
x_train = coerceClass(x_train, c("crew", "seat"), fun = as.factor)

```



Code nominal variables to start at 0 instead of 1.

```{r}
char_vars = names(x_train)[sapply(x_train, is.factor)]
x_train = x_train[, (char_vars) := lapply(.SD, multiCodeVars),
                  .SDcols = char_vars]

```




create xgb.DMatrix for tuning
```{r}

train_ind = sample.int(n = nrow(x_train), size = .6 * nrow(x_train),
                       replace = FALSE)
# val_ind = setdiff(1:nrow(x_train), train_ind)

# for xgboost

# train_small = xgb.DMatrix(data.matrix(x_train[train_ind]),
#                           label = multiCodeVars(y_train[train_ind]$event))
# 
# val = xgb.DMatrix(data.matrix(x_train[!train_ind]),
#                   label = multiCodeVars(y_train[!train_ind]$event))
```

```{r }

# for random forest

train_small = x_train[train_ind]

y_train_small = y_train[train_ind]$event

val = x_train[!train_ind]

y_val =y_train[!train_ind]$event

# for space, remove x_train and y_train
rm(x_train, y_train)
```


Fit random forest model on train_small

```{r}

rf = randomForest::randomForest(x = train_small,
                                y = as.factor(y_train_small),
                                ntree = 100,
                                importance = TRUE)
```


Fit svm

```{r}

```


train xgboost model

### Hyperparameter Tuning

create tuning grid

```{r}
tuneGrid = expand.grid(
  list(
    eta = c(.2),
    gamma = c(0, .2),
    max_depth = c(3, 10),
    subsample = c(.5, .8),
    colsample_bytree = c(.5, .9),
    alpha = c(0, .3),
    min_child_weight = c(.8, 1.5)
  )
)
```

Run models for each parameter combination

```{r}
nclass = length(unique(y_train$event))

# create vector of class labels to calculate multi class log loss
val_labels = multiCodeVars(y_train[!train_ind]$event)


tuning = lapply(seq_len(nrow(tuneGrid)), function(i){
  
  # create list of parameters
  params = list(
    "eta" = tuneGrid$eta[i],
    "objective" = "multi:softprob",
    "eval_metric" = "mlogloss",
    "num_class" = nclass,
    "gamma" = tuneGrid$gamma[i],
    "max_depth" = tuneGrid$max_depth[i],
    "subsample" = tuneGrid$subsample[i],
    "colsample_bytree" = tuneGrid$colsample_bytree[i],
    "alpha" = tuneGrid$alpha[i],
    "min_child_weight" = tuneGrid$min_child_weight[i]
  )
  
  # train model
  model = xgb.train(
    data = train_small,
    params = params,
    nrounds = 10
    )
  
  # make predictions
  preds = formatXgbProbs(model, newdat = val)
  
  # status update
  cat(paste(i, "out of", nrow(tuneGrid), "models complete.\n"))

  # return results from this iteration of 
  return(
    data.table(
    eta = tuneGrid$eta[i],
    gamma = tuneGrid$gamma[i],
    max_depth = tuneGrid$max_depth[i],
    subsample = tuneGrid$subsample[i],
    colsample_bytree = tuneGrid$colsample_bytree[i],
    alpha = tuneGrid$alpha[i],
    min_child_weight = tuneGrid$min_child_weight[i],
    mLogLoss = MLmetrics::MultiLogLoss(y_pred = preds,
                        y_true = val_labels)
    )
  )
  
})

# combine list of results to find the best paramaters
results = data.table::rbindlist(tuning)

# order by performance
results = results[order(mLogLoss)]

# write results to disk
data.table::fwrite(results, "data/modeling_results.csv")


# get best tune
best_tune = results[1]
```


Here, we can further tune the model, but for now let's make another prediction to
submit to kaggle.

```{r}

```

We are able to see that deeper trees with more iterations do better, and increasing the minimum split loss gives a slight edge. 


***

Now that the optimal parameters have been chosen, retrain the model on the entire training dataset.

```{r}
full_train = xgb.DMatrix(data.matrix(x_train),
                         label = multiCodeVars(y_train$event))

full_params = list(
  "objective" = "multi:softprob",
  "eval_metric" = "mlogloss",
  "num_class" = nclass,
  "eta" = best_tune$eta,
  "gamma" = best_tune$gamma,
  "max_depth" = best_tune$max_depth,
  "tree_method" = "exact"
  )
  
full_model = xgb.train(data = full_train,
                       nrounds = 15,
                       params = full_params)

# remove unneeded objects for space
rm(list = ls()[!ls() %in% c("full_model", "full_params", "best_tune")])
```

***

Now that the model has been trained, read in the test data and predict on test.

```{r process test data}

test = fread("data/test.csv")
```


We will initialize a data.table to store our predictions
```{r}
test_preds = test[, "id"]
```


```{r }
# get rid of experiment column
test[, experiment := NULL]

# coerce classes to factor to code them properly
test = coerceClass(test, c("crew", "seat"), fun = as.factor)

# code nominal variables to start at 0
char_vars = names(test)[sapply(test, is.factor)]
test = test[, (char_vars) := lapply(.SD, multiCodeVars),
                  .SDcols = char_vars]


# convert test to a xgb.Dmatrix
test = xgb.DMatrix(data = data.matrix(test[, !"id"]))
```

Finally predict on the test set and create predictions file.

```{r}
probs = formatXgbProbs(full_model, newdat = test)

test_results = setDT(cbind(test_preds, probs))
```

Write predictions to disk to be submitted

```{r}
fwrite(x = test_results, "data/test_results.csv")
```

